{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549e8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8995f789-fe6f-4bcf-b78a-697219bad808;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.2.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.2.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 465ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.2.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.2.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8995f789-fe6f-4bcf-b78a-697219bad808\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/15ms)\n",
      "25/01/13 04:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Row(Symbol='D11',  Open=10400.0,  High=10400.0,  Low=10000.0,  Close=10000.0,  Volume=2800,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='D2D',  Open=31700.0,  High=31700.0,  Low=30700.0,  Close=31500.0,  Volume=16435,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DAC',  Open=4500.0,  High=4500.0,  Low=4500.0,  Close=4500.0,  Volume=0,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DCR',  Open=5900.0,  High=5900.0,  Low=5900.0,  Close=5900.0,  Volume=0,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DAD',  Open=19300.0,  High=19300.0,  Low=19208.0,  Close=19208.0,  Volume=8900,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DAE',  Open=15500.0,  High=15500.0,  Low=15500.0,  Close=15500.0,  Volume=100,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DAG',  Open=1400.0,  High=1400.0,  Low=1400.0,  Close=1400.0,  Volume=0,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DCG',  Open=12000.0,  High=12000.0,  Low=12000.0,  Close=12000.0,  Volume=0,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DAH',  Open=3620.0,  High=3700.0,  Low=3400.0,  Close=3400.0,  Volume=280517,  Trading date=datetime.datetime(2025, 1, 6, 0, 0)), Row(Symbol='DRG',  Open=7800.0,  High=7800.0,  Low=7800.0,  Close=7800.0,  Volume=100,  Trading date=datetime.datetime(2025, 1, 6, 0, 0))]\n",
      "+------+-------+-------+-------+-------+-------+-------------------+\n",
      "|Symbol|   Open|   High|    Low|  Close| Volume|       Trading date|\n",
      "+------+-------+-------+-------+-------+-------+-------------------+\n",
      "|   D11|10400.0|10400.0|10000.0|10000.0|   2800|2025-01-06 00:00:00|\n",
      "|   D2D|31700.0|31700.0|30700.0|31500.0|  16435|2025-01-06 00:00:00|\n",
      "|   DAC| 4500.0| 4500.0| 4500.0| 4500.0|      0|2025-01-06 00:00:00|\n",
      "|   DCR| 5900.0| 5900.0| 5900.0| 5900.0|      0|2025-01-06 00:00:00|\n",
      "|   DAD|19300.0|19300.0|19208.0|19208.0|   8900|2025-01-06 00:00:00|\n",
      "|   DAE|15500.0|15500.0|15500.0|15500.0|    100|2025-01-06 00:00:00|\n",
      "|   DAG| 1400.0| 1400.0| 1400.0| 1400.0|      0|2025-01-06 00:00:00|\n",
      "|   DCG|12000.0|12000.0|12000.0|12000.0|      0|2025-01-06 00:00:00|\n",
      "|   DAH| 3620.0| 3700.0| 3400.0| 3400.0| 280517|2025-01-06 00:00:00|\n",
      "|   DRG| 7800.0| 7800.0| 7800.0| 7800.0|    100|2025-01-06 00:00:00|\n",
      "|   DRI|12600.0|12700.0|12100.0|12377.0| 592015|2025-01-06 00:00:00|\n",
      "|   UDL|12000.0|12000.0|12000.0|12000.0|      0|2025-01-06 00:00:00|\n",
      "|   DWC|14200.0|14200.0|14200.0|14200.0|    421|2025-01-06 00:00:00|\n",
      "|   DHB| 9200.0| 9200.0| 8800.0| 8847.0|   7378|2025-01-06 00:00:00|\n",
      "|   ADS| 8800.0| 8800.0| 8700.0| 8700.0|  61512|2025-01-06 00:00:00|\n",
      "|   DAN|30800.0|30800.0|30800.0|30800.0|      1|2025-01-06 00:00:00|\n",
      "|   DNE| 9400.0| 9400.0| 9400.0| 9400.0|      9|2025-01-06 00:00:00|\n",
      "|   DAS| 6000.0| 6000.0| 6000.0| 6000.0|      0|2025-01-06 00:00:00|\n",
      "|   DPG|46500.0|46900.0|44000.0|44700.0| 420745|2025-01-06 00:00:00|\n",
      "|   DBC|27850.0|27950.0|25850.0|26500.0|6259061|2025-01-06 00:00:00|\n",
      "+------+-------+-------+-------+-------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Writing data to Cassandra...\n",
      "25/01/13 04:26:46 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.16: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:26:46 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (172.18.0.16 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:26:53 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.16: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:26:53 WARN TaskSetManager: Lost task 0.1 in stage 4.0 (TID 5) (172.18.0.16 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/13 04:26:59 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.16: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:26:59 WARN TaskSetManager: Lost task 0.2 in stage 4.0 (TID 6) (172.18.0.16 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:27:06 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.16: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:27:06 WARN TaskSetManager: Lost task 0.3 in stage 4.0 (TID 7) (172.18.0.16 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/13 04:27:06 ERROR TaskSetManager: Task 0 in stage 4.0 failed 4 times; aborting job\n",
      "25/01/13 04:27:06 ERROR AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@77efdac3,com.datastax.spark.connector.cql.CassandraConnector@233df140,TableDef(stock,stock_data,ArrayBuffer(ColumnDef(symbol,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(trading_date,ClusteringColumn(0,ASC),TimestampType)),Stream(ColumnDef(close,RegularColumn,DoubleType), ColumnDef(high,RegularColumn,DoubleType), ColumnDef(low,RegularColumn,DoubleType), ColumnDef(open,RegularColumn,DoubleType), ColumnDef(volume,RegularColumn,DoubleType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(symbol,StringType,true), StructField(trading_date,TimestampType,true), StructField(high,DoubleType,true), StructField(low,DoubleType,true), StructField(open,DoubleType,true), StructField(close,DoubleType,true), StructField(volume,IntegerType,true)),org.apache.spark.SparkConf@549d1c63) is aborting.\n",
      "25/01/13 04:27:06 ERROR AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@77efdac3,com.datastax.spark.connector.cql.CassandraConnector@233df140,TableDef(stock,stock_data,ArrayBuffer(ColumnDef(symbol,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(trading_date,ClusteringColumn(0,ASC),TimestampType)),Stream(ColumnDef(close,RegularColumn,DoubleType), ColumnDef(high,RegularColumn,DoubleType), ColumnDef(low,RegularColumn,DoubleType), ColumnDef(open,RegularColumn,DoubleType), ColumnDef(volume,RegularColumn,DoubleType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(symbol,StringType,true), StructField(trading_date,TimestampType,true), StructField(high,DoubleType,true), StructField(low,DoubleType,true), StructField(open,DoubleType,true), StructField(close,DoubleType,true), StructField(volume,IntegerType,true)),org.apache.spark.SparkConf@549d1c63) aborted.\n",
      "An error occurred while writing data to Cassandra: An error occurred while calling o86.save.\n",
      ": org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.18.0.16 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n",
      "\t... 33 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python spark.py 2025/1/13/stockData.1736742215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad46a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
